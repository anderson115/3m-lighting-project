# Phase 2: Data Collection Plan - Brand Perceptions Module

**Date:** 2025-10-30
**Status:** IN DEVELOPMENT
**Budget:** $0-25 maximum
**Strategy:** Free-first with strategic Bright Data backfill

---

## ğŸ¯ Three-Pass Strategy

### **PASS 1: FREE TOOLS ONLY** (Current Phase)
Collect as much data as possible with 100% free tools, assess coverage gaps

### **PASS 2: ASSESSMENT**
Evaluate what we captured, identify critical gaps that require paid services

### **PASS 3: BACKFILL**
Use Bright Data ($0-25 budget) to fill only the critical gaps identified in Pass 2

---

## Pass 1: Free Data Collection Architecture

### Design Principles
1. **Simple & Stable:** Use battle-tested tools, avoid experimental projects
2. **Checkpoint-Driven:** Verify data quality at every step with small samples
3. **Right-Sized:** No over-engineering, but production-ready
4. **Scalable:** Start with 1 brand sample, then scale to 5 brands

### Tool Selection (Free Only)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PASS 1: FREE DATA COLLECTION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ AMAZON REVIEWS                                           â”‚
â”‚ â€¢ Tool: PRAW-style approach (Reddit API) NOT AVAILABLE   â”‚
â”‚ â€¢ REVISED: Start with WebFetch manual collection        â”‚
â”‚ â€¢ Checkpoint: Verify 10 reviews from 1 product          â”‚
â”‚ â€¢ Scale: If stable, collect 25% sample (5 brands)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ YOUTUBE VIDEOS                                           â”‚
â”‚ â€¢ Tool: YouTube Data API v3 (official, 10k units/day)   â”‚
â”‚ â€¢ Tool: youtube-transcript-api (transcripts)            â”‚
â”‚ â€¢ Checkpoint: Verify 5 videos with transcripts          â”‚
â”‚ â€¢ Scale: 50-100 videos per brand                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ REDDIT DISCUSSIONS                                       â”‚
â”‚ â€¢ Tool: PRAW (Python Reddit API Wrapper - official)     â”‚
â”‚ â€¢ Checkpoint: Verify 10 posts/comments                  â”‚
â”‚ â€¢ Scale: 200-400 posts per brand                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TWITTER/X POSTS                                          â”‚
â”‚ â€¢ Tool: Snscrape (most stable free option 2025)         â”‚
â”‚ â€¢ Checkpoint: Verify 10 tweets                          â”‚
â”‚ â€¢ Scale: 500-1000 tweets per brand                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TIKTOK VIDEOS                                            â”‚
â”‚ â€¢ Tool: davidteather/TikTok-Api                         â”‚
â”‚ â€¢ Checkpoint: Verify 5 videos with metadata            â”‚
â”‚ â€¢ Scale: 100-200 videos per brand                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DEFERRED TO PASS 3 (BRIGHT DATA BACKFILL IF NEEDED)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Amazon reviews (if WebFetch insufficient)             â”‚
â”‚ â€¢ Home Depot reviews                                     â”‚
â”‚ â€¢ Lowes reviews                                          â”‚
â”‚ â€¢ Walmart reviews                                        â”‚
â”‚ â€¢ Menards reviews                                        â”‚
â”‚ â€¢ Ace Hardware reviews                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Implementation Plan

### Stage 1: Setup & Preflight (30 minutes)
- [ ] Install free tool dependencies
- [ ] Configure API credentials (YouTube, Reddit)
- [ ] Create test scripts for each source
- [ ] Run preflight checks on small sample (1 brand, 10 data points each)
- [ ] Git push after preflight passes

### Stage 2: Checkpoint 1 - Single Brand Sample (1 hour)
**Brand:** Command (highest priority)
**Sample Size:** 10 data points per source

**Tasks:**
- [ ] Collect 10 Reddit posts about Command hooks
- [ ] Collect 5 YouTube videos about Command hooks
- [ ] Collect 10 Twitter/X posts about Command hooks
- [ ] Collect 5 TikTok videos about Command hooks
- [ ] Manual WebFetch: 10 Amazon reviews for Command product

**Data Quality Checks:**
- [ ] Verify no blank/null fields in critical columns (text, date, author)
- [ ] Verify US geography (where identifiable)
- [ ] Verify date range (24 months target)
- [ ] Verify data structure consistency

**STOP POINT:** Review sample data with user before proceeding

### Stage 3: Checkpoint 2 - Multi-Brand Small Sample (2 hours)
**Brands:** Command, Scotch-Brite (2 brands)
**Sample Size:** 25 data points per source per brand

**Tasks:**
- [ ] Scale Reddit collection: 25 posts Ã— 2 brands = 50 posts
- [ ] Scale YouTube collection: 10 videos Ã— 2 brands = 20 videos
- [ ] Scale Twitter/X collection: 25 posts Ã— 2 brands = 50 posts
- [ ] Scale TikTok collection: 10 videos Ã— 2 brands = 20 videos

**Data Quality Checks:**
- [ ] Check for duplicate data across brands
- [ ] Verify temporal distribution (not all from same month)
- [ ] Verify brand mentions are accurate (not false positives)
- [ ] Check data volume consistency across brands

**STOP POINT:** Review multi-brand sample, assess stability

### Stage 4: Checkpoint 3 - Full Free Data Collection (4-6 hours)
**Brands:** All 5 (Command, Scotch-Brite, Scotch, Post-it, Scotchgard)
**Sample Size:** 25% sampling target

**Tasks:**
- [ ] Reddit: 100 posts Ã— 5 brands = 500 posts
- [ ] YouTube: 25 videos Ã— 5 brands = 125 videos
- [ ] Twitter/X: 200 posts Ã— 5 brands = 1,000 posts
- [ ] TikTok: 40 videos Ã— 5 brands = 200 videos

**Data Quality Checks:**
- [ ] Verify total record counts match expectations
- [ ] Check for data collection failures/errors
- [ ] Verify US-only filtering (post-process if needed)
- [ ] Verify 24-month temporal filtering

**STOP POINT:** Assess coverage gaps, calculate Pass 2 metrics

---

## Pass 2: Gap Assessment

### Metrics to Calculate
1. **Coverage by Source:**
   - Reddit: X posts collected (target: 500)
   - YouTube: X videos collected (target: 125)
   - Twitter/X: X posts collected (target: 1,000)
   - TikTok: X videos collected (target: 200)
   - Retailer reviews: 0 (target: ~2,000+)

2. **Coverage by Brand:**
   - Command: X% complete
   - Scotch-Brite: X% complete
   - Scotch: X% complete
   - Post-it: X% complete
   - Scotchgard: X% complete

3. **Critical Gaps:**
   - Retailer reviews (Amazon, Home Depot, Lowes, Walmart, Menards, Ace)
   - Any source with <50% target coverage

4. **Cost Estimate for Backfill:**
   - Bright Data requests needed: X
   - Estimated cost: $X (must be â‰¤$25)

---

## Pass 3: Strategic Backfill (Bright Data)

### Budget Allocation ($25 maximum)
**Bright Data Pricing:** 5,000 free requests/month, then ~$0.0009/record

**Strategy:** Use free 5,000 requests first, then paid if needed

### Prioritization (if budget tight):
1. **Tier 1 (Highest Priority):** Amazon reviews
2. **Tier 2 (High Priority):** Home Depot, Lowes reviews
3. **Tier 3 (Medium Priority):** Walmart reviews
4. **Tier 4 (Lower Priority):** Menards, Ace Hardware reviews

### Backfill Plan (TBD after Pass 2 assessment)
- [ ] Bright Data MCP setup
- [ ] Checkpoint: Test 10 retailer reviews
- [ ] Scale: Collect based on priority tiers within budget
- [ ] Final data quality check

---

## Data Quality Checkpoints (Throughout All Passes)

### Checkpoint Script Requirements
```python
# Each checkpoint must verify:
1. Record count matches expectation
2. No critical null/blank fields:
   - text/content (review text, tweet text, etc.)
   - date/timestamp
   - source identifier
3. US geography filtering (where applicable)
4. 24-month temporal filtering
5. Brand mention accuracy (no false positives)
6. Data structure consistency (same schema)
7. No duplicate records
```

### Red Flags to Watch For
- âŒ Entire columns blank (schema mismatch)
- âŒ All dates from same day (scraper hitting cache)
- âŒ Duplicate records with identical IDs
- âŒ Brand name not found in text (false positive)
- âŒ Non-US geography detected (UK, Australia, etc.)
- âŒ Data older than 24 months
- âŒ API rate limit errors (need to throttle)

---

## Git Workflow

### Commit Checkpoints
1. **After Preflight:** "feat: Pass 1 data collection infrastructure with preflight tests"
2. **After Checkpoint 1:** "feat: Single brand sample collection (Command, 10 per source)"
3. **After Checkpoint 2:** "feat: Multi-brand sample collection (2 brands, 25 per source)"
4. **After Checkpoint 3:** "feat: Full free data collection (5 brands, Pass 1 complete)"
5. **After Pass 2:** "docs: Gap assessment and Bright Data backfill plan"
6. **After Pass 3:** "feat: Strategic backfill complete, final data quality verified"

### Branch Strategy
- Main branch: `main`
- Development: Create `phase2-data-collection` branch
- Merge to main only after each checkpoint passes

---

## File Structure

```
modules/brand-perceptions/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ collect_reddit.py          # PRAW Reddit scraper
â”‚   â”œâ”€â”€ collect_youtube.py         # YouTube API + transcripts
â”‚   â”œâ”€â”€ collect_twitter.py         # Snscrape Twitter scraper
â”‚   â”œâ”€â”€ collect_tiktok.py          # TikTok API scraper
â”‚   â”œâ”€â”€ checkpoint_validator.py    # Data quality checks
â”‚   â””â”€â”€ backfill_brightdata.py     # Bright Data MCP integration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ pass1_free/            # Free data collection
â”‚   â”‚   â”‚   â”œâ”€â”€ reddit/
â”‚   â”‚   â”‚   â”œâ”€â”€ youtube/
â”‚   â”‚   â”‚   â”œâ”€â”€ twitter/
â”‚   â”‚   â”‚   â””â”€â”€ tiktok/
â”‚   â”‚   â””â”€â”€ pass3_backfill/        # Bright Data backfill
â”‚   â”‚       â”œâ”€â”€ amazon/
â”‚   â”‚       â”œâ”€â”€ homedepot/
â”‚   â”‚       â””â”€â”€ lowes/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ consolidated/          # Merged, cleaned data
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ data_sources.yaml          # Source configuration
â”‚   â””â”€â”€ checkpoint_rules.yaml      # Validation rules
â””â”€â”€ logs/
    â”œâ”€â”€ preflight_YYYYMMDD.log
    â”œâ”€â”€ checkpoint1_YYYYMMDD.log
    â””â”€â”€ checkpoint2_YYYYMMDD.log
```

---

## Success Criteria

### Pass 1 Success
- âœ… 500+ Reddit posts collected (5 brands)
- âœ… 125+ YouTube videos collected
- âœ… 1,000+ Twitter/X posts collected
- âœ… 200+ TikTok videos collected
- âœ… All checkpoints pass data quality validation
- âœ… Zero stability issues (no crashes, hangs, or data corruption)
- âœ… US-only and 24-month filters applied

### Pass 2 Success
- âœ… Gap assessment complete with metrics
- âœ… Backfill cost estimate â‰¤$25
- âœ… Prioritization plan for Bright Data usage

### Pass 3 Success
- âœ… Retailer review data backfilled within budget
- âœ… Total data cost â‰¤$25
- âœ… Final data quality validation passes
- âœ… 50+ verbatim quotes per brand minimum achieved

---

## Next Steps

1. **User approval of this plan**
2. **Begin Stage 1: Setup & Preflight**
3. **Checkpoint 1: Single brand sample**
4. **Stop and report back for evaluation**

---

## Notes

- **Stability > Speed:** Will use conservative API rate limits to avoid bans
- **Small samples first:** Never scale until checkpoint passes
- **User in the loop:** Stop at every checkpoint for evaluation
- **Cost tracking:** Monitor Bright Data usage in real-time to stay under $25
- **Rollback ready:** Git commits at each checkpoint allow rollback if issues found
