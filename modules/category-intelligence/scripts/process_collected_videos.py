#!/usr/bin/env python3
"""
Bridge script: Download collected social videos and process through consumer-video pipeline
Routes outputs to category-intelligence folder structure
"""

import json
import sys
import os
from pathlib import Path
from datetime import datetime
import subprocess

# Setup paths
PROJECT_ROOT = Path(__file__).parent.parent.parent
CATEGORY_INTEL_ROOT = Path(__file__).parent
CONSUMER_VIDEO_ROOT = PROJECT_ROOT / "modules" / "consumer-video"

# Add consumer-video to path to use its processing functions
sys.path.insert(0, str(CONSUMER_VIDEO_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / "core"))

print("=" * 80)
print("SOCIAL VIDEO PROCESSING PIPELINE")
print("Collect ‚Üí Download ‚Üí Process ‚Üí Extract Insights")
print("=" * 80)

# Configuration
VIDEO_SOURCE = CATEGORY_INTEL_ROOT / "data" / "social_videos" / "youtube_3m_claw_FINAL_20251104_194358.json"
# VIDEO_SOURCE = CATEGORY_INTEL_ROOT / "data" / "social_videos" / "TEST_THREE_VIDEOS.json"  # TEST MODE
OUTPUT_BASE = CATEGORY_INTEL_ROOT / "data" / "processed_videos"
OUTPUT_BASE.mkdir(parents=True, exist_ok=True)

# Create organized output structure
DIRS = {
    "raw_videos": OUTPUT_BASE / "raw_videos",
    "audio": OUTPUT_BASE / "audio",
    "transcripts": OUTPUT_BASE / "transcripts",
    "frames": OUTPUT_BASE / "frames",
    "insights": OUTPUT_BASE / "insights",
    "reports": OUTPUT_BASE / "reports"
}

for dir_path in DIRS.values():
    dir_path.mkdir(parents=True, exist_ok=True)

print(f"\nüìÅ Output Structure:")
print(f"   Base: {OUTPUT_BASE}")
for name, path in DIRS.items():
    print(f"   ‚îú‚îÄ‚îÄ {name}/")

# Load collected videos
print(f"\nüì• Loading collected videos from:")
print(f"   {VIDEO_SOURCE.name}")

with open(VIDEO_SOURCE) as f:
    data = json.load(f)

videos = data['videos']
print(f"\n‚úì Found {len(videos)} videos to process")

# User confirmation
print("\n" + "=" * 80)
print("PROCESSING PLAN")
print("=" * 80)
print(f"\nThis will:")
print(f"  1. Download {len(videos)} YouTube videos")
print(f"  2. Extract audio (FFmpeg)")
print(f"  3. Transcribe (Whisper large-v3)")
print(f"  4. Extract frames (OpenCV)")
print(f"  5. Extract insights (GPT-4)")
print(f"\nEstimated time: {len(videos) * 10} minutes ({len(videos)} videos √ó 10 min/video)")
print(f"Storage required: ~{len(videos) * 50} MB")
print(f"\n‚úì Auto-starting processing pipeline...")

# Skip interactive prompt for automation

# Processing loop
# Use yt-dlp command directly instead of YouTubeDataSource to avoid API key requirement

successful = 0
failed = 0
skipped = 0

processing_log = []

for i, video in enumerate(videos, 1):
    video_id = video.get('video_id')
    title = video.get('title', 'Unknown')[:60]
    url = video.get('url', '')

    # Skip videos with no video_id
    if not video_id:
        print(f"\n{'='*80}")
        print(f"[{i}/{len(videos)}] {title}")
        print(f"{'='*80}")
        print(f"‚ö†Ô∏è  SKIPPING: No video_id")
        skipped += 1
        continue

    print(f"\n{'='*80}")
    print(f"[{i}/{len(videos)}] {title}")
    print(f"{'='*80}")
    print(f"Video ID: {video_id}")
    print(f"URL: {url}")

    # Create video-specific output directory
    video_output_dir = OUTPUT_BASE / "individual" / video_id
    video_output_dir.mkdir(parents=True, exist_ok=True)

    result = {
        "video_id": video_id,
        "title": title,
        "url": url,
        "processed_at": datetime.now().isoformat(),
        "status": {}
    }

    try:
        # Step 1: Download video
        print(f"\n[1/5] üì• Downloading video...")
        video_path = DIRS["raw_videos"] / f"{video_id}.mp4"

        if video_path.exists():
            print(f"   ‚è≠Ô∏è  Video already downloaded")
            result["status"]["download"] = "‚úì (cached)"
        else:
            # Download using yt-dlp command
            ytdlp_cmd = [
                "yt-dlp",
                "-f", "best[ext=mp4]",
                "--no-playlist",
                "--no-warnings",
                "-o", str(DIRS["raw_videos"] / f"{video_id}.%(ext)s"),
                url
            ]
            subprocess.run(ytdlp_cmd, capture_output=True, check=True)

            if video_path.exists():
                print(f"   ‚úì Downloaded: {video_path.name}")
                result["status"]["download"] = "‚úì"
            else:
                raise Exception("Download failed")

        # Step 2: Extract audio
        print(f"\n[2/5] üéµ Extracting audio...")
        audio_path = DIRS["audio"] / f"{video_id}.wav"

        if audio_path.exists():
            print(f"   ‚è≠Ô∏è  Audio already extracted")
            result["status"]["audio"] = "‚úì (cached)"
        else:
            # Extract audio using ffmpeg
            ffmpeg_cmd = [
                "ffmpeg",
                "-i", str(video_path),
                "-vn",
                "-acodec", "pcm_s16le",
                "-ar", "16000",
                "-ac", "1",
                str(audio_path),
                "-y"
            ]
            subprocess.run(ffmpeg_cmd, capture_output=True, check=True)

            if audio_path.exists():
                print(f"   ‚úì Extracted: {audio_path.name}")
                result["status"]["audio"] = "‚úì"
            else:
                raise Exception("Audio extraction failed")

        # Step 3: Transcribe
        print(f"\n[3/5] üìù Transcribing (Whisper)...")
        transcript_path = DIRS["transcripts"] / f"{video_id}.json"

        if transcript_path.exists():
            print(f"   ‚è≠Ô∏è  Transcript already exists")
            result["status"]["transcript"] = "‚úì (cached)"
        else:
            import whisper
            import ssl
            import urllib.request
            # Disable SSL verification for model download
            ssl._create_default_https_context = ssl._create_unverified_context
            # Load model from default cache (~/.cache/whisper) to avoid TARS permission issues
            model = whisper.load_model("large-v3")
            transcript_result = model.transcribe(str(audio_path), language="en")

            transcript_data = {
                "video_id": video_id,
                "title": title,
                "text": transcript_result["text"],
                "segments": [{
                    "start": s["start"],
                    "end": s["end"],
                    "text": s["text"]
                } for s in transcript_result["segments"]],
                "word_count": len(transcript_result["text"].split()),
                "duration": transcript_result["segments"][-1]["end"] if transcript_result["segments"] else 0
            }

            with open(transcript_path, 'w') as f:
                json.dump(transcript_data, f, indent=2)

            print(f"   ‚úì Transcribed: {len(transcript_data['segments'])} segments, {transcript_data['word_count']} words")
            result["status"]["transcript"] = "‚úì"
            result["word_count"] = transcript_data['word_count']

        # Step 4: Extract frames
        print(f"\n[4/5] üñºÔ∏è  Extracting frames...")
        frames_dir = DIRS["frames"] / video_id
        frames_dir.mkdir(exist_ok=True)

        existing_frames = list(frames_dir.glob("*.jpg"))
        if len(existing_frames) > 0:
            print(f"   ‚è≠Ô∏è  Frames already extracted ({len(existing_frames)} frames)")
            result["status"]["frames"] = f"‚úì (cached, {len(existing_frames)} frames)"
        else:
            import cv2
            cap = cv2.VideoCapture(str(video_path))
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_interval = 5  # Extract every 5 seconds

            frame_count = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0
                if int(current_time) % frame_interval == 0:
                    frame_path = frames_dir / f"frame_{int(current_time):04d}s.jpg"
                    cv2.imwrite(str(frame_path), frame)
                    frame_count += 1

            cap.release()
            print(f"   ‚úì Extracted {frame_count} frames (every {frame_interval}s)")
            result["status"]["frames"] = f"‚úì {frame_count} frames"
            result["frame_count"] = frame_count

        # Step 5: Extract insights (using GPT-4) - SKIPPED (no API key)
        print(f"\n[5/5] üí° Extracting insights...")
        insights_path = DIRS["insights"] / f"{video_id}.json"

        if insights_path.exists():
            print(f"   ‚è≠Ô∏è  Insights already extracted")
            result["status"]["insights"] = "‚úì (cached)"
        else:
            # SKIP GPT-4 insights for now (requires OPENAI_API_KEY)
            print(f"   ‚è≠Ô∏è  Skipping GPT-4 insights (no API key)")
            result["status"]["insights"] = "‚è≠Ô∏è  skipped"

        if False:  # Disabled GPT-4 code
            # Load transcript
            with open(transcript_path) as f:
                transcript_data = json.load(f)

            # Use GPT-4 for insight extraction
            from openai import OpenAI
            client = OpenAI()

            prompt = f"""Analyze this video transcript for consumer insights about "{title}".

Transcript:
{transcript_data['text']}

Extract:
1. **Main Topic/Purpose**: What is this video about?
2. **Product/Brand Mentioned**: What products or brands are discussed?
3. **Pain Points**: What problems or frustrations are mentioned?
4. **Solutions/Benefits**: What solutions or benefits are highlighted?
5. **Consumer Language**: What specific words/phrases does the speaker use?
6. **Key Quotes**: 2-3 most important direct quotes with timestamps

Format as JSON."""

            response = client.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": "You are a consumer insights analyst extracting structured insights from video transcripts."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"}
            )

            insights = json.loads(response.choices[0].message.content)
            insights["video_id"] = video_id
            insights["title"] = title
            insights["processed_at"] = datetime.now().isoformat()

            with open(insights_path, 'w') as f:
                json.dump(insights, f, indent=2)

            print(f"   ‚úì Insights extracted")
            result["status"]["insights"] = "‚úì"

        # Success
        successful += 1
        result["overall_status"] = "‚úì SUCCESS"
        print(f"\n‚úÖ Video {i}/{len(videos)} complete")

    except Exception as e:
        failed += 1
        result["overall_status"] = f"‚úó FAILED: {str(e)}"
        print(f"\n‚ùå Error: {str(e)}")
        result["error"] = str(e)

    processing_log.append(result)

    # Save progress after each video
    progress_file = OUTPUT_BASE / "processing_log.json"
    with open(progress_file, 'w') as f:
        json.dump({
            "processed_at": datetime.now().isoformat(),
            "total_videos": len(videos),
            "successful": successful,
            "failed": failed,
            "skipped": skipped,
            "videos": processing_log
        }, f, indent=2)

# Final summary
print("\n" + "=" * 80)
print("PROCESSING COMPLETE")
print("=" * 80)
print(f"\nüìä Results:")
print(f"   ‚úì Successful: {successful}/{len(videos)}")
print(f"   ‚úó Failed: {failed}/{len(videos)}")
print(f"   ‚è≠Ô∏è  Skipped: {skipped}/{len(videos)}")

print(f"\nüìÅ Output Locations:")
print(f"   Raw videos: {DIRS['raw_videos']}")
print(f"   Audio: {DIRS['audio']}")
print(f"   Transcripts: {DIRS['transcripts']}")
print(f"   Frames: {DIRS['frames']}")
print(f"   Insights: {DIRS['insights']}")
print(f"   Processing log: {progress_file}")

# Generate summary report
report_path = DIRS["reports"] / f"processing_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
with open(report_path, 'w') as f:
    f.write(f"# Social Video Processing Report\n\n")
    f.write(f"**Generated:** {datetime.now().isoformat()}\n\n")
    f.write(f"## Summary\n\n")
    f.write(f"- Total videos: {len(videos)}\n")
    f.write(f"- Successful: {successful}\n")
    f.write(f"- Failed: {failed}\n")
    f.write(f"- Success rate: {successful/len(videos)*100:.1f}%\n\n")
    f.write(f"## Output Structure\n\n")
    f.write(f"```\n")
    f.write(f"{OUTPUT_BASE.name}/\n")
    for name in DIRS.keys():
        f.write(f"‚îú‚îÄ‚îÄ {name}/\n")
    f.write(f"```\n\n")
    f.write(f"## Processed Videos\n\n")
    for result in processing_log:
        status_emoji = "‚úÖ" if result["overall_status"] == "‚úì SUCCESS" else "‚ùå"
        f.write(f"### {status_emoji} {result['title']}\n\n")
        f.write(f"- Video ID: `{result['video_id']}`\n")
        f.write(f"- URL: {result['url']}\n")
        f.write(f"- Status: {result['overall_status']}\n")
        if result.get('word_count'):
            f.write(f"- Word count: {result['word_count']}\n")
        if result.get('frame_count'):
            f.write(f"- Frames extracted: {result['frame_count']}\n")
        f.write(f"\n")

print(f"\nüìÑ Summary report: {report_path}")

print("\n‚úì Pipeline complete!")
