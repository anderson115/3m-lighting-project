# Category Intelligence Module - Complete Guide

**Version**: 2.0 (Agentic System)
**Status**: Production + Migration
**Last Updated**: 2025-10-16
**Zero Fabrication Policy**: ENFORCED

---

## üìã **TABLE OF CONTENTS**

1. [Module Overview](#module-overview)
2. [Architecture (Dual System)](#architecture-dual-system)
3. [File Structure](#file-structure)
4. [Quick Start](#quick-start)
5. [Component Reference](#component-reference)
6. [Data Sources & APIs](#data-sources--apis)
7. [Coding Standards](#coding-standards)
8. [Development Workflow](#development-workflow)
9. [Testing & Validation](#testing--validation)
10. [Troubleshooting](#troubleshooting)

---

## üìñ **MODULE OVERVIEW**

The Category Intelligence Module generates comprehensive market research reports for product categories with **ZERO fabrication tolerance**. Every data point must be traceable to a real source.

### **Core Capabilities**
- ‚úÖ Market sizing & growth analysis (real data from FRED API, WebSearch)
- ‚úÖ Brand discovery & competitive landscape (50+ brands per category)
- ‚úÖ Product pricing analysis (scraping from 4+ retailers)
- ‚úÖ Learning resource curation (30+ curated sources)
- ‚úÖ Consumer insights (JTBD framework with video evidence)
- ‚úÖ Source tracking & audit trail (100% traceability)
- ‚úÖ Professional HTML report generation (consulting-grade formatting)

### **Target Use Case**
- **Users**: Product managers, strategy consultants, market researchers
- **Input**: Category name (e.g., "garage storage", "smart home lighting")
- **Output**: Comprehensive HTML report with citations + audit trail JSON

---

## üèóÔ∏è **ARCHITECTURE (DUAL SYSTEM)**

### **IMPORTANT: Two Parallel Systems**

This module currently has TWO orchestration systems running in parallel during migration:

#### **System 1: Legacy Pipeline** (CURRENTLY ACTIVE)
- **Entry Point**: `run_analysis.py` ‚Üí `core/orchestrator.py`
- **Collectors**: `collectors/` (brand_discovery.py, market_researcher.py, etc.)
- **Data**: Mix of real + hardcoded data (being migrated)
- **Status**: ‚úÖ Working, ‚ö†Ô∏è Contains hardcoded data, üîÑ Being replaced

#### **System 2: Agentic System** (NEW - IN DEVELOPMENT)
- **Entry Point**: `agents/orchestrator.py` (OrchestratorAgent)
- **Agents**: `agents/` (validators, collectors, base classes)
- **Data**: 100% real data (WebSearch, APIs, scraping)
- **Status**: ‚úÖ Core complete, ‚è≥ Integration in progress

### **Migration Path**
```
Phase 1: ‚úÖ COMPLETE - Validation layer + Orchestrator agent built
Phase 2: ‚è≥ IN PROGRESS - Integrate collectors with validation
Phase 3: ‚è≥ PENDING - Replace legacy collectors with agent-based
Phase 4: ‚è≥ PENDING - Update run_analysis.py to use agentic system
Phase 5: ‚è≥ PENDING - Deprecate legacy system
```

---

## üìÅ **FILE STRUCTURE**

```
modules/category_intelligence/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ MODULE_GUIDE.md          ‚Üê YOU ARE HERE (LLM entry point)
‚îú‚îÄ‚îÄ üìÑ README.md                ‚Üê User-facing documentation
‚îú‚îÄ‚îÄ üìÑ IMPLEMENTATION_CHECKLIST.md ‚Üê Development progress tracker
‚îÇ
‚îú‚îÄ‚îÄ üìö ARCHITECTURE DOCS
‚îÇ   ‚îú‚îÄ‚îÄ AGENTIC_ARCHITECTURE.md ‚Üê Agent system design (NEW)
‚îÇ   ‚îú‚îÄ‚îÄ DATA_SOURCE_MAPPING.md  ‚Üê All data sources & acquisition methods
‚îÇ   ‚îú‚îÄ‚îÄ SCRAPING_ARCHITECTURE.md ‚Üê Web scraping infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ DATA_STORAGE.md         ‚Üê Data persistence strategy
‚îÇ   ‚îî‚îÄ‚îÄ PRD.md                  ‚Üê Product requirements (historical)
‚îÇ
‚îú‚îÄ‚îÄ ü§ñ agents/                  ‚Üê NEW AGENTIC SYSTEM
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base.py                 ‚Üê Agent base classes, message protocol (307 lines)
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py         ‚Üê Master coordinator, ACCEPT/REJECT/REFINE (472 lines)
‚îÇ   ‚îú‚îÄ‚îÄ validators.py           ‚Üê 4 validation agents (700 lines)
‚îÇ   ‚îî‚îÄ‚îÄ collectors.py           ‚Üê Data collection agents (312 lines)
‚îÇ
‚îú‚îÄ‚îÄ üì¶ collectors/              ‚Üê LEGACY COLLECTORS (being migrated)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ brand_discovery.py      ‚Üê Discovers 50+ brands (819 lines) ‚ö†Ô∏è LARGE
‚îÇ   ‚îú‚îÄ‚îÄ market_researcher.py    ‚Üê Market size & growth (836 lines) ‚ö†Ô∏è LARGE
‚îÇ   ‚îú‚îÄ‚îÄ pricing_analyzer.py     ‚Üê Product pricing (703 lines) ‚ö†Ô∏è LARGE
‚îÇ   ‚îú‚îÄ‚îÄ resource_curator.py     ‚Üê Learning resources (509 lines)
‚îÇ   ‚îú‚îÄ‚îÄ taxonomy_builder.py     ‚Üê Product categorization (437 lines)
‚îÇ   ‚îî‚îÄ‚îÄ consumer_insights.py    ‚Üê JTBD analysis (567 lines)
‚îÇ
‚îú‚îÄ‚îÄ ‚öôÔ∏è core/                    ‚Üê SHARED INFRASTRUCTURE
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py               ‚Üê Configuration management (110 lines)
‚îÇ   ‚îú‚îÄ‚îÄ source_tracker.py       ‚Üê Source validation & audit (166 lines)
‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.py         ‚Üê Legacy orchestrator (253 lines) ‚ö†Ô∏è OLD SYSTEM
‚îÇ
‚îú‚îÄ‚îÄ üìä generators/              ‚Üê REPORT GENERATION
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ html_reporter.py        ‚Üê Professional HTML reports (804 lines) ‚ö†Ô∏è LARGE
‚îÇ
‚îú‚îÄ‚îÄ üõ†Ô∏è utils/                   ‚Üê UTILITIES
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ data_storage.py         ‚Üê Data persistence helpers (271 lines)
‚îÇ
‚îú‚îÄ‚îÄ üì§ outputs/                 ‚Üê GENERATED REPORTS
‚îÇ   ‚îú‚îÄ‚îÄ audit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ source_audit.json   ‚Üê All sources with validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ citations.json      ‚Üê Citation list
‚îÇ   ‚îú‚îÄ‚îÄ garage_storage_final_Category_Intelligence.html
‚îÇ   ‚îî‚îÄ‚îÄ garage_storage_consulting_Category_Intelligence.html
‚îÇ
‚îî‚îÄ‚îÄ üöÄ run_analysis.py          ‚Üê CLI entry point (95 lines)
```

### **‚ö†Ô∏è Files Marked LARGE (>700 lines)**
These files contain extensive data and may need refactoring:
- `collectors/market_researcher.py` (836 lines) - Contains hardcoded market data
- `collectors/brand_discovery.py` (819 lines) - Contains hardcoded brand lists
- `html_reporter.py` (804 lines) - Acceptable for complex report generation
- `pricing_analyzer.py` (703 lines) - Contains hardcoded pricing data
- `validators.py` (700 lines) - 4 validators, could split but OK

---

## üöÄ **QUICK START**

### **Prerequisites**
```bash
# Python 3.13+
python --version

# Install dependencies
pip install pydantic requests  # Core dependencies
pip install "scrapling[fetchers]"  # For web scraping (optional)

# Set environment variables (optional)
export FRED_API_KEY="your_fred_api_key"  # For economic data
```

### **Run Analysis (Current System)**
```bash
cd /Users/anderson115/00-interlink/12-work/3m-lighting-project/modules/category_intelligence

# Run analysis
python run_analysis.py --category "garage storage"

# With custom output name
python run_analysis.py --category "smart home lighting" --output "lighting_report"
```

### **Output Files**
```
outputs/
‚îú‚îÄ‚îÄ {category}_Category_Intelligence.html  # Main report
‚îî‚îÄ‚îÄ audit/
    ‚îú‚îÄ‚îÄ source_audit.json                  # All sources
    ‚îî‚îÄ‚îÄ citations.json                     # Citation list
```

---

## üß© **COMPONENT REFERENCE**

### **LEGACY COLLECTORS** (collectors/)

#### **BrandDiscovery** (`brand_discovery.py`)
```python
from modules.category_intelligence.collectors.brand_discovery import BrandDiscovery

brand_discovery = BrandDiscovery(config)
brands = brand_discovery.discover_brands("garage storage")

# Returns:
# {
#     "status": "completed",
#     "brands": [
#         {"name": "Rubbermaid", "tier": "tier_1_national", ...},
#         ...
#     ],
#     "total_brands": 50+
# }
```
**Current Status**: ‚ö†Ô∏è Contains hardcoded brand data. Migration to real WebSearch pending.

#### **MarketResearcher** (`market_researcher.py`)
```python
from modules.category_intelligence.collectors.market_researcher import MarketResearcher

researcher = MarketResearcher(config)
market_size = researcher.analyze_market_size("garage storage")
market_share = researcher.research_market_share("garage storage")

# Returns market size, growth, projections, competitive landscape
```
**Current Status**: ‚ö†Ô∏è Contains hardcoded market data. Migration to FRED API + WebSearch pending.

#### **PricingAnalyzer** (`pricing_analyzer.py`)
```python
from modules.category_intelligence.collectors.pricing_analyzer import PricingAnalyzer

pricing = PricingAnalyzer(config)
pricing_data = pricing.analyze_pricing("garage storage")

# Returns pricing ranges, products, retailers, top brands
```
**Current Status**: ‚ö†Ô∏è Contains hardcoded pricing. Migration to Scrapling web scraping pending.

#### **ConsumerInsightsCollector** (`consumer_insights.py`)
```python
from modules.category_intelligence.collectors.consumer_insights import ConsumerInsightsCollector

insights = ConsumerInsightsCollector(config)
jtbd_analysis = insights.analyze_jobs_to_be_done(video_data)

# Returns JTBD framework with evidence from consumer videos
```
**Current Status**: ‚úÖ Production-ready for video-based JTBD analysis.

---

### **NEW AGENTIC SYSTEM** (agents/)

#### **OrchestratorAgent** (`orchestrator.py`)
```python
from modules.category_intelligence.agents.orchestrator import OrchestratorAgent

# Initialize with requirements
requirements = {
    "brands": {"min_count": 50},
    "market_size": {"required": True},
    "pricing": {"min_products": 120, "min_retailers": 4}
}

orchestrator = OrchestratorAgent(category="garage storage", requirements=requirements)

# Assign task to collector
await orchestrator.assign_task(market_agent, "market_size", parameters={})

# Receive and validate submission
decision = await orchestrator.receive_submission(submission)
# Returns: OrchestratorDecision (ACCEPT, REJECT, or REFINE)

# Get progress
report = orchestrator.get_progress_report()
# Returns: completeness %, gaps, quality distribution
```

#### **Validation Agents** (`validators.py`)

**QualityValidationAgent**: Checks data completeness, format, consistency, recency
```python
from modules.category_intelligence.agents.validators import QualityValidationAgent

validator = QualityValidationAgent("quality_validator")
result = await validator.validate(submission)
# Returns: ValidationResult (passed=True/False, score 0.0-1.0, issues, recommendations)
```

**SourceValidationAgent**: ZERO FABRICATION ENFORCEMENT
```python
from modules.category_intelligence.agents.validators import SourceValidationAgent

validator = SourceValidationAgent("source_validator")
result = await validator.validate(submission)
# Rejects if: No sources, placeholder URLs, fabrication markers detected
# Returns: score=1.0 required to pass
```

**RelevanceValidationAgent**: Category relevance check
```python
from modules.category_intelligence.agents.validators import RelevanceValidationAgent

validator = RelevanceValidationAgent("relevance_validator", category="garage storage")
result = await validator.validate(submission)
# Checks: keyword matching, brand relevance, product relevance
```

**GapIdentificationAgent**: Identifies missing data
```python
from modules.category_intelligence.agents.validators import GapIdentificationAgent

analyzer = GapIdentificationAgent("gap_analyzer")
gaps = await analyzer.analyze_gaps(collected_data, requirements)
# Returns: List[Gap] with priority, suggested actions
```

---

## üîó **DATA SOURCES & APIs**

See `DATA_SOURCE_MAPPING.md` for comprehensive list. Key sources:

### **Working Data Sources**
1. **WebSearch (Claude API)** - Market intelligence, brand discovery
2. **FRED API** - Retail sales time series (`RSFSN`, `RSBMGESD`)
3. **Census Bureau API** - Retail trade data
4. **CORGIS Datasets** - Retail services data (downloaded)
5. **Web Scraping (Scrapling)** - Product pricing from retailers

### **API Keys Required**
```bash
# Optional - for enhanced data collection
export FRED_API_KEY="your_key_here"  # Get from https://fred.stlouisfed.org/
```

### **Web Scraping Infrastructure**
- **Tool**: Scrapling 0.3.7 + Camoufox browser
- **Success Rate**: 92-95% against anti-bot systems
- **Retailers**: Home Depot, Lowe's, Amazon, Walmart
- **Architecture**: See `SCRAPING_ARCHITECTURE.md`

---

## üíª **CODING STANDARDS**

### **Zero Fabrication Policy**
```python
# ‚ùå NEVER DO THIS
data = {
    "market_size": "$3.5B",
    "source": "example.com/placeholder"
}

# ‚úÖ ALWAYS DO THIS
from agents.base import Source

data = {
    "market_size": "$3.5B",
    "sources": [
        Source(
            url="https://www.grandviewresearch.com/industry-analysis/garage-storage-market",
            publisher="Grand View Research",
            date_published="2024-09-15",
            excerpt="The US garage storage market was valued at $3.5 billion in 2024...",
            confidence="high"
        )
    ]
}
```

### **Data Submission Format**
```python
from agents.base import DataSubmission, Source

submission = DataSubmission(
    sender_agent="market_data_agent",
    recipient_agent="orchestrator",
    data_type="market_size",
    data={"market_size_usd": 3.5e9, "year": 2024},
    sources=[Source(...)],  # REQUIRED
    confidence=0.85,
    quality_self_assessment=0.9,
    reasoning="Collected from Grand View Research report"
)

# Validate no fabrication
issues = submission.validate_no_fabrication()
if issues:
    raise ValueError(f"Fabrication detected: {issues}")
```

### **File Size Guidelines**
- **Small**: < 300 lines (single responsibility)
- **Medium**: 300-500 lines (acceptable)
- **Large**: 500-800 lines (consider refactoring)
- **Too Large**: > 800 lines (MUST refactor unless justified)

### **Naming Conventions**
- **Classes**: `PascalCase` (e.g., `MarketDataAgent`)
- **Functions**: `snake_case` (e.g., `collect_data`)
- **Constants**: `UPPER_SNAKE_CASE` (e.g., `MAX_RETRIES`)
- **Private methods**: `_leading_underscore` (e.g., `_validate_sources`)

### **Documentation Requirements**
```python
def collect_data(self, category: str, **kwargs) -> Dict[str, Any]:
    """
    Collect market data for category using WebSearch.

    Args:
        category: Category name (e.g., "garage storage")
        **kwargs: Additional parameters

    Returns:
        Dict with structure:
        {
            "data": {...},
            "sources": [Source(...)],
            "confidence": 0.0-1.0,
            "quality_score": 0.0-1.0,
            "reasoning": str
        }

    Raises:
        ValueError: If no sources found or fabrication detected
    """
```

---

## üîÑ **DEVELOPMENT WORKFLOW**

### **Adding a New Collector**
1. Create agent class inheriting from `CollectionAgent`
2. Implement `collect_data()` with real data sources
3. Add source tracking for every data point
4. Test with orchestrator validation
5. Update `IMPLEMENTATION_CHECKLIST.md`

### **Adding a New Validator**
1. Create agent class inheriting from `ValidationAgent`
2. Implement `validate()` method
3. Return `ValidationResult` with score, issues, recommendations
4. Add to `OrchestratorAgent` validation flow
5. Test with sample data

### **Migration Checklist for Legacy Collectors**
```markdown
- [ ] Identify all hardcoded data in collector
- [ ] Map to real data sources (see DATA_SOURCE_MAPPING.md)
- [ ] Implement WebSearch/API integration
- [ ] Add Source tracking for every data point
- [ ] Wrap in DataSubmission format
- [ ] Remove ALL hardcoded data
- [ ] Test with SourceValidator (must pass)
- [ ] Verify zero fabrication confirmed
```

---

## üß™ **TESTING & VALIDATION**

### **Source Validation Test**
```python
# Test that all data has sources
from agents.validators import SourceValidationAgent

validator = SourceValidationAgent("test")
result = await validator.validate(submission)

assert result.passed == True, f"Source validation failed: {result.issues}"
assert result.score == 1.0, "Source validation requires perfect score"
```

### **Quality Metrics**
Target metrics for production reports:
- ‚úÖ **Completeness**: >= 95%
- ‚úÖ **Average Quality Score**: >= 0.90
- ‚úÖ **Source Traceability**: 100%
- ‚úÖ **Data Recency**: All from 2023-2025
- ‚úÖ **Brand Count**: >= 50
- ‚úÖ **Pricing Products**: >= 120 (4 retailers √ó 30)
- ‚úÖ **Resources**: >= 30

### **Running Validation**
```python
# Check source audit
import json
with open("outputs/audit/source_audit.json") as f:
    audit = json.load(f)

print(f"Total sources: {audit['total_sources']}")
print(f"High confidence: {audit['by_confidence']['high']}")
print(f"Fabrication markers: {audit['fabrication_check']['markers_found']}")

# Should be 0 fabrication markers
assert audit['fabrication_check']['markers_found'] == 0
```

---

## üêõ **TROUBLESHOOTING**

### **Common Issues**

#### **Source Validation Failing**
```
Error: "CRITICAL: No sources provided"
Solution: Every data point MUST have Source object with URL
```

#### **Fabrication Marker Detected**
```
Error: "Fabrication marker detected: 'placeholder'"
Solution: Remove all placeholder text, example.com URLs, TODO markers
```

#### **Quality Score Too Low**
```
Error: "Validation score 0.65 < 0.7 threshold"
Solution:
  1. Check data completeness (all required fields present?)
  2. Verify data recency (2023-2025 preferred)
  3. Add more high-confidence sources
```

#### **Legacy System Not Working**
```
Error: "ModuleNotFoundError: No module named 'modules.category_intelligence'"
Solution: Run from correct directory or add to PYTHONPATH
```

---

## üìö **RELATED DOCUMENTATION**

- **AGENTIC_ARCHITECTURE.md** - Complete agent system design
- **DATA_SOURCE_MAPPING.md** - All verified data sources
- **SCRAPING_ARCHITECTURE.md** - Web scraping infrastructure
- **IMPLEMENTATION_CHECKLIST.md** - Development progress (Stages 1-7)
- **README.md** - User-facing documentation

---

## üìä **CURRENT STATUS**

| Component | Status | Lines | Notes |
|-----------|--------|-------|-------|
| Validation Layer | ‚úÖ Complete | 700 | All 4 validators working |
| Orchestrator Agent | ‚úÖ Complete | 472 | ACCEPT/REJECT/REFINE logic |
| Collection Agents | ‚è≥ Partial | 312 | Structure ready, integration pending |
| Legacy Collectors | ‚ö†Ô∏è Working | 3,200+ | Contains hardcoded data |
| Report Generator | ‚úÖ Complete | 804 | Professional formatting |
| Documentation | ‚úÖ Complete | - | LLM-optimized |

**Overall Progress**: 29% (Stages 1-2 of 7 complete)

---

## üéØ **NEXT STEPS FOR LLMS**

If you're an LLM working on this module:

1. **Read this file first** - It's the single source of truth
2. **Check IMPLEMENTATION_CHECKLIST.md** - See what's done/pending
3. **Review AGENTIC_ARCHITECTURE.md** - Understand the new system
4. **Reference DATA_SOURCE_MAPPING.md** - For data collection methods
5. **Follow coding standards** - Zero fabrication, source tracking
6. **Update IMPLEMENTATION_CHECKLIST.md** - After completing work
7. **Test with validators** - Ensure quality before committing

---

**Last Updated**: 2025-10-16
**Maintained By**: Development Team
**License**: Proprietary
