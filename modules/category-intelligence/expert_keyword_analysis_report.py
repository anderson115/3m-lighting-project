#!/usr/bin/env python3
"""
EXPERT-LEVEL CATEGORY KEYWORD ANALYSIS REPORT
Top 2% Quality - Strategic Market Intelligence

Analyzes the language gap between retailers and consumers to identify:
1. Hidden opportunity keywords
2. Consumer intent taxonomy
3. Competitive keyword strategy
4. Subcategory ecosystem mapping
5. Jobs-To-Be-Done keyword mapping

Data Sources:
- 12,929 retailer products
- 1,300+ consumer sources (Reddit, YouTube, TikTok)
- 187K retailer keywords, 66K consumer keywords
"""

import json
from pathlib import Path
from collections import Counter, defaultdict
import statistics

# Load comprehensive keyword analysis
data_file = Path(__file__).parent / "outputs" / "comprehensive_keyword_analysis_full.json"
with open(data_file) as f:
    keyword_data = json.load(f)

print("="*80)
print("EXPERT KEYWORD ANALYSIS - GARAGE ORGANIZER CATEGORY")
print("Strategic Market Intelligence Report")
print("="*80)
print()

# Extract frequency data
retailer_unigrams = dict(keyword_data['retailer_keywords']['unigrams_top_100'])
retailer_bigrams = dict(keyword_data['retailer_keywords']['bigrams_top_100'])
retailer_trigrams = dict(keyword_data['retailer_keywords']['trigrams_top_100'])

consumer_unigrams = dict(keyword_data['consumer_keywords']['unigrams_top_100'])
consumer_bigrams = dict(keyword_data['consumer_keywords']['bigrams_top_100'])
consumer_trigrams = dict(keyword_data['consumer_keywords']['trigrams_top_100'])

# ============================================================================
# ANALYSIS 1: LANGUAGE GAP ANALYSIS
# ============================================================================

print("ANALYSIS 1: RETAILER VS CONSUMER LANGUAGE GAP")
print("-" * 80)
print()

# Calculate relative frequency ratios
def calculate_consumer_preference_score(keyword, consumer_freq, retailer_freq):
    """
    Score indicating consumer preference vs retailer emphasis.
    Positive = consumers use more than retailers
    Negative = retailers use more than consumers
    """
    if consumer_freq == 0 and retailer_freq == 0:
        return 0

    # Normalize by corpus size (approximate)
    consumer_norm = consumer_freq / 66000  # consumer corpus size
    retailer_norm = retailer_freq / 187000  # retailer corpus size

    if retailer_norm == 0:
        return 100  # Pure consumer keyword

    ratio = (consumer_norm / retailer_norm) * 100
    return ratio

# Find consumer-heavy keywords (high consumer use, low retailer use)
consumer_heavy = []
for keyword, consumer_freq in consumer_bigrams.items():
    retailer_freq = retailer_bigrams.get(keyword, 0)
    score = calculate_consumer_preference_score(keyword, consumer_freq, retailer_freq)

    # Only include if consumer uses it at least 20 times
    if consumer_freq >= 20:
        consumer_heavy.append({
            'keyword': keyword,
            'consumer_freq': consumer_freq,
            'retailer_freq': retailer_freq,
            'preference_score': score
        })

consumer_heavy.sort(key=lambda x: x['preference_score'], reverse=True)

print("üîç HIGH OPPORTUNITY KEYWORDS (Consumer-Heavy)")
print("Keywords consumers use heavily but retailers underutilize:")
print()

for i, kw in enumerate(consumer_heavy[:20], 1):
    print(f"{i:2d}. '{kw['keyword']}'")
    print(f"    Consumer mentions: {kw['consumer_freq']:,}")
    print(f"    Retailer mentions: {kw['retailer_freq']:,}")
    print(f"    Opportunity score: {kw['preference_score']:.1f}x")
    print()

# Find retailer-heavy keywords (retailers saturate, consumers less interested)
retailer_heavy = []
for keyword, retailer_freq in retailer_bigrams.items():
    consumer_freq = consumer_bigrams.get(keyword, 0)
    score = calculate_consumer_preference_score(keyword, consumer_freq, retailer_freq)

    # Only include if retailer uses it heavily (500+)
    if retailer_freq >= 500:
        retailer_heavy.append({
            'keyword': keyword,
            'consumer_freq': consumer_freq,
            'retailer_freq': retailer_freq,
            'preference_score': score
        })

retailer_heavy.sort(key=lambda x: x['preference_score'])

print()
print("‚ö†Ô∏è  OVERSATURATED KEYWORDS (Retailer-Heavy)")
print("Keywords retailers emphasize but consumers don't seek:")
print()

for i, kw in enumerate(retailer_heavy[:15], 1):
    print(f"{i:2d}. '{kw['keyword']}'")
    print(f"    Retailer mentions: {kw['retailer_freq']:,}")
    print(f"    Consumer mentions: {kw['consumer_freq']:,}")
    print(f"    Saturation score: {kw['preference_score']:.1f}x")
    print()

print()

# ============================================================================
# ANALYSIS 2: CONSUMER JOBS-TO-BE-DONE KEYWORD MAPPING
# ============================================================================

print("ANALYSIS 2: CONSUMER JOBS-TO-BE-DONE KEYWORD MAPPING")
print("-" * 80)
print()

# Categorize keywords by job type
functional_jobs = [
    'hang', 'organize', 'store', 'mount', 'install', 'hold', 'keep',
    'storage', 'organization', 'hanging', 'mounting'
]

emotional_jobs = [
    'clean', 'neat', 'beautiful', 'nice', 'perfect', 'dream', 'love',
    'amazing', 'awesome', 'transformation', 'makeover'
]

social_jobs = [
    'diy', 'easy', 'simple', 'affordable', 'budget', 'cheap',
    'looks', 'show', 'share'
]

# Count job-related keywords
job_keywords = defaultdict(list)

for keyword, freq in consumer_bigrams.items():
    # Functional jobs
    if any(fj in keyword for fj in functional_jobs):
        job_keywords['functional'].append((keyword, freq))

    # Emotional jobs
    if any(ej in keyword for ej in emotional_jobs):
        job_keywords['emotional'].append((keyword, freq))

    # Social jobs
    if any(sj in keyword for sj in social_jobs):
        job_keywords['social'].append((keyword, freq))

print("FUNCTIONAL JOBS (What needs to be done)")
print("Top keywords indicating functional needs:")
print()
for i, (kw, freq) in enumerate(sorted(job_keywords['functional'], key=lambda x: x[1], reverse=True)[:10], 1):
    print(f"  {i:2d}. '{kw}' ‚Üí {freq} mentions")
print()

print("EMOTIONAL JOBS (How consumers want to feel)")
print("Top keywords indicating emotional outcomes:")
print()
for i, (kw, freq) in enumerate(sorted(job_keywords['emotional'], key=lambda x: x[1], reverse=True)[:10], 1):
    print(f"  {i:2d}. '{kw}' ‚Üí {freq} mentions")
print()

print("SOCIAL JOBS (How consumers want to be perceived)")
print("Top keywords indicating social motivations:")
print()
for i, (kw, freq) in enumerate(sorted(job_keywords['social'], key=lambda x: x[1], reverse=True)[:10], 1):
    print(f"  {i:2d}. '{kw}' ‚Üí {freq} mentions")
print()

# ============================================================================
# ANALYSIS 3: SUBCATEGORY KEYWORD ECOSYSTEMS
# ============================================================================

print("ANALYSIS 3: SUBCATEGORY KEYWORD ECOSYSTEMS")
print("-" * 80)
print()

# Identify subcategory clusters
subcategories = {
    'Wall Storage Systems': ['slatwall', 'pegboard', 'french cleat', 'wall mount', 'wall storage', 'grid wall'],
    'Hooks & Hangers': ['hooks', 'heavy duty', 'utility hook', 'bike hook', 'ladder hook', 'hanger'],
    'Shelving': ['shelf', 'shelving', 'shelves', 'rack', 'storage rack', 'wire shelf'],
    'Cabinets': ['cabinet', 'garage cabinet', 'storage cabinet', 'locker', 'tool cabinet'],
    'Workbenches': ['workbench', 'work bench', 'work table', 'tool storage', 'work surface'],
    'Overhead Storage': ['overhead', 'ceiling', 'ceiling storage', 'overhead rack', 'ceiling rack'],
    'Bins & Containers': ['bin', 'container', 'tote', 'storage bin', 'stackable']
}

print("Subcategory Keyword Analysis:")
print()

for subcat, keywords in subcategories.items():
    consumer_total = 0
    retailer_total = 0

    for kw in keywords:
        # Check unigrams
        consumer_total += consumer_unigrams.get(kw, 0)
        retailer_total += retailer_unigrams.get(kw, 0)

        # Check bigrams containing keyword
        for bigram, freq in consumer_bigrams.items():
            if kw in bigram:
                consumer_total += freq

        for bigram, freq in retailer_bigrams.items():
            if kw in bigram:
                retailer_total += freq

    preference = calculate_consumer_preference_score(subcat, consumer_total, retailer_total)

    print(f"üì¶ {subcat}")
    print(f"   Consumer volume: {consumer_total:,} mentions")
    print(f"   Retailer volume: {retailer_total:,} mentions")
    print(f"   Consumer preference: {preference:.1f}x")

    if preference > 80:
        print(f"   ‚≠ê HIGH OPPORTUNITY - Consumers highly interested")
    elif preference < 20:
        print(f"   ‚ö†Ô∏è  LOW INTEREST - Retailer oversupply")
    else:
        print(f"   ‚úì Balanced market")

    print()

# ============================================================================
# ANALYSIS 4: COMPETITIVE KEYWORD STRATEGY
# ============================================================================

print("ANALYSIS 4: COMPETITIVE KEYWORD STRATEGY")
print("-" * 80)
print()

print("BRAND & RETAILER MENTIONS IN CONSUMER LANGUAGE:")
print()

# Extract brand/retailer mentions from consumer keywords
brands = ['ikea', 'rubbermaid', 'gladiator', 'kobalt', 'craftsman',
          'husky', 'closetmaid', 'gorilla', 'NewAge']

retailers = ['home depot', 'homedepot', 'lowes', 'amazon', 'costco',
             'harbor freight', 'menards', 'walmart']

brand_mentions = []
for brand in brands:
    count = 0
    for keyword, freq in consumer_unigrams.items():
        if brand.lower() in keyword.lower():
            count += freq
    if count > 0:
        brand_mentions.append((brand, count))

retailer_mentions = []
for retailer in retailers:
    count = 0
    for keyword, freq in consumer_bigrams.items():
        if retailer.lower() in keyword.lower():
            count += freq
    if count > 0:
        retailer_mentions.append((retailer, count))

print("Top Brands in Consumer Conversations:")
for brand, count in sorted(brand_mentions, key=lambda x: x[1], reverse=True):
    print(f"  ‚Ä¢ {brand.title()}: {count:,} mentions")
print()

print("Top Retailers in Consumer Conversations:")
for retailer, count in sorted(retailer_mentions, key=lambda x: x[1], reverse=True):
    print(f"  ‚Ä¢ {retailer.title()}: {count:,} mentions")
print()

# ============================================================================
# ANALYSIS 5: EMERGING TRENDS & NICHE OPPORTUNITIES
# ============================================================================

print("ANALYSIS 5: EMERGING TRENDS & NICHE OPPORTUNITIES")
print("-" * 80)
print()

# Identify unique consumer keywords (appear in consumer but not retailer top 100)
emerging_keywords = []
for keyword, freq in consumer_bigrams.items():
    if keyword not in retailer_bigrams and freq >= 15:
        emerging_keywords.append((keyword, freq))

emerging_keywords.sort(key=lambda x: x[1], reverse=True)

print("üöÄ EMERGING CONSUMER TRENDS (Not yet in retailer vocabulary):")
print()
for i, (kw, freq) in enumerate(emerging_keywords[:25], 1):
    print(f"  {i:2d}. '{kw}' ‚Üí {freq} consumer mentions, {retailer_bigrams.get(kw, 0)} retailer mentions")
print()

# ============================================================================
# SAVE STRATEGIC REPORT
# ============================================================================

report_data = {
    "report_type": "expert_keyword_strategic_analysis",
    "quality_tier": "top_2_percent",
    "analyses": {
        "language_gap": {
            "high_opportunity_keywords": consumer_heavy[:20],
            "oversaturated_keywords": retailer_heavy[:15]
        },
        "jobs_to_be_done": {
            "functional_jobs": sorted(job_keywords['functional'], key=lambda x: x[1], reverse=True)[:10],
            "emotional_jobs": sorted(job_keywords['emotional'], key=lambda x: x[1], reverse=True)[:10],
            "social_jobs": sorted(job_keywords['social'], key=lambda x: x[1], reverse=True)[:10]
        },
        "subcategory_ecosystems": {
            subcat: {
                "keywords": keywords,
                "consumer_volume": sum(consumer_unigrams.get(kw, 0) for kw in keywords),
                "retailer_volume": sum(retailer_unigrams.get(kw, 0) for kw in keywords)
            }
            for subcat, keywords in subcategories.items()
        },
        "emerging_trends": [
            {"keyword": kw, "frequency": freq}
            for kw, freq in emerging_keywords[:25]
        ],
        "brand_competitive_intelligence": {
            "brand_mentions": [{"brand": b, "mentions": c} for b, c in sorted(brand_mentions, key=lambda x: x[1], reverse=True)],
            "retailer_mentions": [{"retailer": r, "mentions": c} for r, c in sorted(retailer_mentions, key=lambda x: x[1], reverse=True)]
        }
    },
    "data_quality": {
        "retailer_products": keyword_data['metadata']['data_sources']['retailer_products'],
        "consumer_sources": keyword_data['metadata']['data_sources']['reddit_posts'] +
                           keyword_data['metadata']['data_sources']['youtube_videos'] +
                           keyword_data['metadata']['data_sources']['tiktok_videos'],
        "retailer_keywords_analyzed": keyword_data['metadata']['corpus_stats']['retailer_unigrams'],
        "consumer_keywords_analyzed": keyword_data['metadata']['corpus_stats']['consumer_unigrams']
    }
}

output_file = Path(__file__).parent / "outputs" / "expert_keyword_strategic_report.json"
with open(output_file, 'w') as f:
    json.dump(report_data, f, indent=2)

print()
print("="*80)
print("STRATEGIC REPORT COMPLETE")
print("="*80)
print(f"Full report saved to: {output_file.name}")
print()
print("KEY INSIGHTS:")
print(f"  ‚Ä¢ {len(consumer_heavy)} high-opportunity keywords identified")
print(f"  ‚Ä¢ {len(retailer_heavy)} oversaturated keywords identified")
print(f"  ‚Ä¢ {len(emerging_keywords)} emerging trend keywords discovered")
print(f"  ‚Ä¢ 7 subcategory ecosystems mapped")
print(f"  ‚Ä¢ {len(brand_mentions)} brands tracked in consumer conversations")
print()
print("NEXT STEPS:")
print("  1. Focus content on high-opportunity keywords")
print("  2. De-emphasize oversaturated keywords")
print("  3. Monitor emerging trends for early mover advantage")
print("  4. Develop subcategory-specific strategies")
print("="*80)
