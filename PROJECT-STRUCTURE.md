# 3M LIGHTING PROJECT - STRUCTURE & STANDARDS

**Project:** Multi-Module Research Automation Platform
**Client:** 3M Lighting
**Date:** 2025-10-08
**Status:** Modular Architecture Established

---

## ðŸ“ PROJECT STRUCTURE

```
3m-lighting-project/
â”œâ”€â”€ modules/                          # All analysis modules (modular architecture)
â”‚   â”œâ”€â”€ youtube-datasource/           # âœ… PRODUCTION
â”‚   â”‚   â”œâ”€â”€ scripts/                  # multimodal_analyzer.py, run_preflight_analysis.py
â”‚   â”‚   â”œâ”€â”€ data/3m_lighting/         # Processed YouTube videos
â”‚   â”‚   â”œâ”€â”€ config/                   # Symlink to shared config
â”‚   â”‚   â”œâ”€â”€ docs/                     # Module documentation
â”‚   â”‚   â”œâ”€â”€ tests/                    # Module tests
â”‚   â”‚   â””â”€â”€ README.md                 # Module overview
â”‚   â”‚
â”‚   â”œâ”€â”€ consumer-video/               # ðŸ“‹ PLANNED (ready for dev)
â”‚   â”‚   â”œâ”€â”€ scripts/                  # consumer_analyzer.py, emotion_analyzer.py
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â”œâ”€â”€ raw_videos/           # Client-provided interviews (gitignored)
â”‚   â”‚   â”‚   â”œâ”€â”€ processed/            # Per-video analysis JSON
â”‚   â”‚   â”‚   â””â”€â”€ deliverables/         # Final reports
â”‚   â”‚   â”œâ”€â”€ prompts/                  # Domain-specific extraction prompts
â”‚   â”‚   â”œâ”€â”€ config/                   # Symlink to shared config
â”‚   â”‚   â”œâ”€â”€ docs/                     # Implementation plans
â”‚   â”‚   â”œâ”€â”€ tests/                    # Validation scripts
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â””â”€â”€ creator-discovery/            # ðŸ“‹ PLANNED (PRD complete)
â”‚       â”œâ”€â”€ scripts/                  # Discovery, profiling, classification
â”‚       â”œâ”€â”€ data/                     # Creator databases
â”‚       â”œâ”€â”€ config/                   # API keys, search params
â”‚       â”œâ”€â”€ docs/                     # PRD and specifications
â”‚       â”œâ”€â”€ tests/                    # Validation tests
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ config/                           # SHARED CONFIGURATION
â”‚   â””â”€â”€ model_paths.yaml              # Model locations on TARS volume
â”‚
â”œâ”€â”€ scripts/                          # LEGACY/SHARED SCRIPTS (being migrated to modules)
â”‚   â”œâ”€â”€ multimodal_analyzer.py        # Core analyzer (copied to youtube module)
â”‚   â”œâ”€â”€ run_preflight_analysis.py     # YouTube analysis orchestrator
â”‚   â””â”€â”€ checkpoint_test_5videos.py    # Validation test
â”‚
â”œâ”€â”€ data/                             # LEGACY DATA (being migrated to modules)
â”‚   â”œâ”€â”€ 3m_lighting/                  # â†’ modules/youtube-datasource/data/
â”‚   â”œâ”€â”€ checkpoint_test_5videos/      # Test data
â”‚   â”œâ”€â”€ checkpoint_test_output/       # Test results
â”‚   â””â”€â”€ consumer-interviews/          # â†’ modules/consumer-video/data/raw_videos/
â”‚
â”œâ”€â”€ docs/                             # PROJECT DOCUMENTATION
â”‚   â”œâ”€â”€ README.md                     # Main project overview
â”‚   â”œâ”€â”€ PROJECT-STRUCTURE.md          # This file
â”‚   â”œâ”€â”€ consumer-video-FINAL-PLAN.md  # Consumer video implementation plan
â”‚   â”œâ”€â”€ PRD-creator-discovery.md      # Creator discovery PRD
â”‚   â”œâ”€â”€ EMOTION-ANALYSIS-BEST-OPTIONS-2025.md  # Emotion model research
â”‚   â”œâ”€â”€ model-price-plan.md           # Model comparison and costs
â”‚   â””â”€â”€ youtube-datasource-README.md  # YouTube module documentation
â”‚
â”œâ”€â”€ tests/                            # PROJECT-LEVEL TESTS
â”‚   â””â”€â”€ (module-specific tests in module folders)
â”‚
â”œâ”€â”€ venv/                             # Python virtual environment
â”œâ”€â”€ requirements.txt                  # Shared dependencies
â”œâ”€â”€ .gitignore                        # Git exclusions (media files)
â”œâ”€â”€ .env.example                      # Environment template
â””â”€â”€ README.md                         # Quick start guide
```

---

## ðŸŽ¯ MODULE STANDARDS

### Required Structure (All Modules)
```
module-name/
â”œâ”€â”€ scripts/          # All Python scripts for this module
â”œâ”€â”€ data/             # Module-specific data (gitignored if large)
â”œâ”€â”€ config/           # Module configuration (symlink to shared or module-specific)
â”œâ”€â”€ docs/             # Module documentation
â”œâ”€â”€ tests/            # Module unit tests
â””â”€â”€ README.md         # Module overview, status, models used
```

### Required Documentation (Each Module)
1. **README.md** - Overview, purpose, status
2. **Status badge** - âœ… Production, ðŸ”„ Development, ðŸ“‹ Planned
3. **Models used** - List all AI models and dependencies
4. **Structure** - Folder descriptions
5. **Quick start** - How to run the module

### Shared Resources
- **config/model_paths.yaml** - All modules symlink to this
- **requirements.txt** - Shared Python dependencies
- **/Volumes/TARS/llm-models/** - All models stored here

---

## ðŸ”§ TECHNOLOGY STACK

### Core Models (Shared)
| Model | Purpose | Location | Size |
|-------|---------|----------|------|
| Whisper large-v3 | Audio transcription | TARS/llm-models/whisper/ | 1.5GB |
| LLaVA 7B | Visual analysis | Ollama (llava:latest) | 4.5GB |
| MiniCPM-V 8B | Vision (optional upgrade) | Ollama (minicpm-v:latest) | 5.5GB |

### Module-Specific Models
| Model | Module | Purpose | Location | Size |
|-------|--------|---------|----------|------|
| SpeechBrain wav2vec2-IEMOCAP | consumer-video | Emotion recognition | TARS/llm-models/speechbrain/ | 460MB |

### Frameworks
- **Python 3.13** - Primary language
- **PyTorch 2.8.0** - ML framework (MPS acceleration)
- **FFmpeg** - Video/audio processing
- **Ollama** - Local LLM serving
- **librosa** - Audio signal processing
- **SpeechBrain** - Speech emotion recognition

---

## ðŸ“Š MODULE STATUS

### âœ… Production Modules
**youtube-datasource**
- Status: Production-ready
- Validation: 96% success rate (5/5 videos, 46.6 min total)
- Models: Whisper large-v3, LLaVA 7B
- Output: Pain points, solutions, JTBD insights
- Performance: ~8-9 min per video

### ðŸ”„ Development Modules
**consumer-video**
- Status: Implementation plan ready
- Plan: docs/consumer-video-FINAL-PLAN.md
- Models: Whisper, LLaVA, SpeechBrain wav2vec2, librosa
- Output: Pain points, emotions, 3M adjacency, workarounds
- Timeline: 12-16 hours development

### ðŸ“‹ Planned Modules
**creator-discovery**
- Status: PRD complete
- PRD: docs/PRD-creator-discovery.md
- APIs: YouTube, TikTok, Instagram
- Output: Creator profiles, demographics, psychographics
- Timeline: 12 weeks (4 phases)

---

## ðŸ”„ MIGRATION STATUS

### Completed
- âœ… Created modular folder structure
- âœ… Module READMEs created
- âœ… Shared config established (model_paths.yaml)
- âœ… Documentation consolidated

### In Progress
- ðŸ”„ Migrating youtube scripts to modules/youtube-datasource/
- ðŸ”„ Migrating data to module folders
- ðŸ”„ Updating import paths

### Pending
- â³ consumer-video module implementation
- â³ creator-discovery module implementation

---

## ðŸš€ QUICK START (By Module)

### YouTube Data Source
```bash
cd modules/youtube-datasource
source ../../venv/bin/activate
python scripts/run_preflight_analysis.py
```

### Consumer Video (Once Implemented)
```bash
cd modules/consumer-video
source ../../venv/bin/activate
python scripts/run_batch_analysis.py --input data/raw_videos/ --output data/processed/
```

### Creator Discovery (Once Implemented)
```bash
cd modules/creator-discovery
source ../../venv/bin/activate
python scripts/discover_creators.py --niche "DIY lighting" --platforms youtube,tiktok
```

---

## ðŸ“‹ DEVELOPMENT STANDARDS

### Code Style
- **PEP 8** compliant
- **Type hints** on all functions
- **Docstrings** for all classes and functions
- **Error handling** with try/except and logging

### Testing
- **Unit tests** in module tests/ folder
- **Integration tests** for full pipeline
- **Validation tests** with known-good data
- **Minimum 70% coverage**

### Documentation
- **README.md** in every module
- **Inline comments** for complex logic
- **CHANGELOG.md** for version tracking
- **API documentation** for public functions

### Git Workflow
- **main** branch for production code
- **Feature branches** for development
- **Descriptive commits** with context
- **Tags** for releases (v1.0.0-module-name)

---

## ðŸ” SECURITY & PRIVACY

### Sensitive Data
- âœ… `.env` files gitignored
- âœ… API keys never committed
- âœ… Consumer videos gitignored (large media files)
- âœ… Personal data scrubbed from outputs

### Model Security
- âœ… All models local (no external API by default)
- âœ… TARS volume for centralized model storage
- âœ… PRO tier API calls optional (user consent required)

---

## ðŸ“¦ DEPENDENCIES

### Current (requirements.txt)
```txt
openai-whisper>=20231117
torch>=2.8.0
torchaudio>=2.8.0
torchvision>=0.19.0
moviepy>=1.0.3
Pillow>=10.0.0
requests>=2.31.0
pyyaml>=6.0
ollama>=0.1.0

# Planned additions for consumer-video
speechbrain>=1.0.0
librosa>=0.10.0
soundfile>=0.12.0
```

### System Requirements
- **Python:** 3.13+
- **RAM:** 32GB minimum (64GB recommended)
- **Storage:** 50GB+ free on TARS volume
- **GPU:** Mac M2/M3 with MPS, or NVIDIA CUDA

---

## ðŸŽ¯ PROJECT GOALS

1. **Modularity** - Each data source is independent module
2. **Reusability** - Shared models and configuration
3. **Maintainability** - Clear structure, consistent docs
4. **Scalability** - Easy to add new modules
5. **Quality** - 80%+ accuracy on all extractions
6. **Speed** - <10 min per video processing time

---

## ðŸ“ž SUPPORT

**Documentation:** See `docs/` folder for detailed guides
**Issues:** Track in GitHub Issues
**Questions:** Check module README.md files first

---

**Last Updated:** 2025-10-08
**Version:** 1.0.0 (modular architecture established)
