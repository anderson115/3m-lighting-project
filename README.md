# 3M Lighting Project
**YouTube Research Automation for Lighting Pain Points**

---

## ğŸ¯ **Overview**

Automated YouTube video analysis platform using multimodal AI to extract Jobs-to-be-Done insights from lighting installation and home improvement content.

**Pipeline:** Whisper large-v3 (audio transcription) + LLaVA 7B (visual analysis) + JTBD extraction

**Current Status:** Production-ready, simplified architecture (HuBERT removed, dependencies reduced 56%)

---

## ğŸš€ **Quick Start**

### **Prerequisites**
- Python 3.11+ (tested on 3.13)
- Ollama (for LLaVA vision model)
- FFmpeg (for video/audio processing)
- 16GB+ RAM (64GB recommended for batch processing)

### **Setup**
```bash
# 1. Clone repository
git clone https://github.com/anderson115/3m-lighting-project.git
cd 3m-lighting-project

# 2. Install Python dependencies
python -m venv venv
source venv/bin/activate  # or: venv\Scripts\activate on Windows
pip install -r requirements.txt

# 3. Install Ollama and pull LLaVA model
brew install ollama  # or download from ollama.com
ollama pull llava:latest

# 4. Verify Ollama is running
ollama list  # Should show llava:latest

# 5. Configure environment (optional for API models)
cp .env.example .env
# Edit .env if using OpenAI/Gemini/Zhipu models
```

### **Run Analysis**
```bash
# Run preflight analysis (3 test videos)
python scripts/run_preflight_analysis.py

# Output: data/preflight_analysis/{video_id}/analysis.json
# Summary: data/preflight_analysis/summary_report.json
```

---

## ğŸ“ **Project Structure**

```
3m-lighting-project/
â”œâ”€â”€ scripts/                          # Analysis pipeline
â”‚   â”œâ”€â”€ run_preflight_analysis.py    # MAIN: Orchestrator for 3 test videos
â”‚   â”œâ”€â”€ multimodal_analyzer.py       # Core: Whisper + LLaVA + JTBD extraction
â”‚   â”œâ”€â”€ video_downloader.py          # YouTube download + frame extraction
â”‚   â”œâ”€â”€ test_local_models.py         # Local model benchmarking
â”‚   â”œâ”€â”€ test_api_comprehensive.py    # API model testing
â”‚   â””â”€â”€ validate_pipeline.py         # End-to-end validation
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ consumer-video/              # âœ… Consumer interview analysis (PRODUCTION)
â”‚   â”‚   â”œâ”€â”€ scripts/                 # JTBD + emotion + product extraction
â”‚   â”‚   â”œâ”€â”€ data/processed/          # Analysis outputs + client deliverables
â”‚   â”‚   â””â”€â”€ README.md                # Full documentation
â”‚   â”œâ”€â”€ youtube-datasource/          # âœ… YouTube video analysis (PRODUCTION)
â”‚   â”‚   â””â”€â”€ scripts/                 # Whisper + LLaVA pipeline
â”‚   â”œâ”€â”€ expert-authority/            # ğŸ“‹ Expert discussion analysis (PLANNED)
â”‚   â”‚   â””â”€â”€ README.md                # Reddit, Quora, forums scraping
â”‚   â”œâ”€â”€ social-signal/               # ğŸ“‹ Visual social analysis (PLANNED)
â”‚   â”‚   â””â”€â”€ README.md                # Pinterest, Instagram, TikTok trends
â”‚   â””â”€â”€ creator-discovery/           # ğŸ“‹ Creator identification (PLANNED)
â”‚       â””â”€â”€ README.md                # Multi-platform creator profiling
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ preflight/                   # Input: Preflight test videos
â”‚   â”‚   â”œâ”€â”€ beginner/                # Beginner-level video
â”‚   â”‚   â”œâ”€â”€ intermediate/            # Intermediate-level video
â”‚   â”‚   â””â”€â”€ advanced/                # Advanced-level video
â”‚   â”œâ”€â”€ preflight_analysis/          # Output: Analysis results
â”‚   â”‚   â”œâ”€â”€ {video_id}/              # Per-video analysis
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.json        # Full multimodal analysis
â”‚   â”‚   â”‚   â”œâ”€â”€ audio.wav            # Extracted audio (16kHz mono)
â”‚   â”‚   â”‚   â””â”€â”€ frames/              # Keyframes (30s intervals)
â”‚   â”‚   â”œâ”€â”€ summary_report.json      # Aggregated insights
â”‚   â”‚   â””â”€â”€ preflight_summary.json   # Performance metrics
â”‚   â””â”€â”€ 3m_lighting/
â”‚       â””â”€â”€ archives/                # Completed analysis runs
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ model_paths.yaml             # Model configuration (Whisper + LLaVA)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ model-price-plan.md          # â­ MASTER: Vision model scorecard
â”‚   â”œâ”€â”€ youtube-datasource-README.md # Complete setup + usage guide
â”‚   â”œâ”€â”€ local-models-comparison.md   # Qualitative model comparisons
â”‚   â””â”€â”€ jtbd-extraction-plan.md      # Jobs-to-be-Done methodology
â”‚
â”œâ”€â”€ requirements.txt                 # Python dependencies (62 packages)
â”œâ”€â”€ .env.example                     # API key template
â””â”€â”€ README.md                        # This file
```

---

## ğŸ”§ **Architecture**

### **Processing Pipeline**

```
1. VIDEO INGESTION
   â””â”€ yt-dlp: YouTube URL â†’ MP4 download

2. PREPROCESSING
   â”œâ”€ ffmpeg: Video â†’ 16kHz mono WAV audio
   â””â”€ ffmpeg: Video â†’ Keyframes (1 frame per 30s)

3. MULTIMODAL ANALYSIS
   â”œâ”€ Whisper large-v3: Audio â†’ Transcript with timestamps
   â””â”€ LLaVA 7B: Frames â†’ Visual analysis (lighting fixtures, installations)

4. JTBD EXTRACTION
   â””â”€ Keyword-based extraction from transcripts + visual data
      â”œâ”€ Pain points (dark, dim, glare, problem, issue)
      â”œâ”€ Solutions (LED, strip light, dimmer, fix, install)
      â”œâ”€ Verbatim quotes (top pain points)
      â””â”€ Timestamp mapping

5. OUTPUT GENERATION
   â”œâ”€ Per-video analysis.json (full multimodal data)
   â”œâ”€ summary_report.json (aggregated insights)
   â””â”€ preflight_summary.json (performance metrics)
```

### **Models Used**

| Model | Purpose | Device | Size | Speed |
|-------|---------|--------|------|-------|
| **Whisper large-v3** | Audio transcription | CPU (MPS backend) | 1.5GB | ~6 min/video |
| **LLaVA 7B** | Visual frame analysis | Ollama (local) | 4.5GB | ~60s/frame |

**Note:** HuBERT emotion analysis was removed (1.2GB savings, no functional impact). System now runs Whisper + LLaVA only.

---

## ğŸ” **Current Status**

### **Preflight Complete âœ…**
- **Videos analyzed:** 3 (beginner, intermediate, advanced)
- **Pain points found:** 11
- **Solutions identified:** 21
- **Verbatim quotes:** 10
- **Processing time:** 25.3 minutes total (8.4 min avg per video)

### **Recent Updates**
- âœ… **2025-10-08:** Simplified architecture (removed HuBERT, -56% dependencies)
- âœ… **2025-10-07:** Model benchmarking complete (9 models tested)
- âœ… **2025-10-05:** Initial preflight run successful

### **Results Location**
```bash
data/preflight_analysis/
â”œâ”€â”€ 6YlrdMaM0dw/          # Beginner: Easy DIY LED Shelf Lighting
â”œâ”€â”€ ZoWPdtYkdCc/          # Intermediate: Baking Polymer Clay
â”œâ”€â”€ IE8iCsXYp_Y/          # Advanced: 10 HomeKit Automations
â”œâ”€â”€ summary_report.json   # Aggregated insights
â””â”€â”€ preflight_summary.json # Performance metrics
```

---

## ğŸ“š **Documentation**

| Document | Purpose |
|----------|---------|
| **`docs/model-price-plan.md`** | â­ Complete vision model scorecard (9 models tested) |
| **`docs/youtube-datasource-README.md`** | Detailed setup guide with troubleshooting |
| **`docs/local-models-comparison.md`** | Qualitative analysis of local models |
| **`docs/model-output-comparison.md`** | Side-by-side model output examples |
| **`README.md`** | This file - quick start and overview |

---

## ğŸ§ª **Validation**

```bash
# Validate complete pipeline
python scripts/validate_pipeline.py

# Test vision models (benchmark 4 local models)
python scripts/test_local_models.py

# Test API models (benchmark 5 API models)
python scripts/test_api_comprehensive.py
```

---

## ğŸ› ï¸ **Troubleshooting**

### **Ollama Issues**
```bash
# Check if Ollama is running
ollama list

# Start Ollama service
ollama serve

# Pull LLaVA model if missing
ollama pull llava:latest

# Test LLaVA with sample image
echo "Describe this image" | ollama run llava:latest < test_image.jpg
```

###  **FFmpeg Not Found**
```bash
# macOS
brew install ffmpeg

# Ubuntu/Debian
sudo apt install ffmpeg

# Verify installation
ffmpeg -version
```

### **Memory Issues**
- **Whisper FP16 warning:** Normal on CPU, automatically uses FP32
- **Out of memory:** Reduce `batch_size` in `config/model_paths.yaml`
- **LLaVA crashes:** Ensure Ollama has sufficient memory (4GB+ available)

### **Import Errors**
```bash
# Reinstall dependencies
pip install -r requirements.txt --force-reinstall

# Check Python version (requires 3.11+)
python --version
```

---

## ğŸ“Š **Performance Notes**

- **Whisper large-v3:** ~6 minutes per 9-minute video (CPU with MPS backend)
- **LLaVA 7B:** ~60 seconds per frame (varies by complexity)
- **Total pipeline:** ~8.4 minutes per video average (3 videos = 25 minutes)
- **Simplified vs original:** -56% dependencies, -15s startup time, same functionality

---

## ğŸ“„ **License**

Proprietary - 3M Lighting Research Project
