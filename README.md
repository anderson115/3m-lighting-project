# Video Research Platform
**Multi-Client YouTube Research Automation**

---

## ğŸ¯ **Overview**

Research automation platform for analyzing YouTube videos using multimodal AI (Whisper + LLaVA + LLM extraction) to identify Jobs-to-be-Done insights, pain points, solutions, and product adjacencies.

**Current Client:** 3M Lighting Division (Command Hook adjacency research)

---

## ğŸš€ **Quick Start**

### **Prerequisites**
- Python 3.11+
- Ollama (running locally)
- FFmpeg
- 64GB RAM (recommended for large models)

### **Setup**
```bash
# Clone and install
git clone <repo>
cd video-research-platform
python -m venv venv
source venv/bin/activate  # or: venv\Scripts\activate on Windows
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with YouTube API key

# Validate setup
python scripts/validate_setup.py
```

### **Run Research**
```bash
# Process videos for a client
python scripts/run_research.py --client 3m_lighting --mode production

# Run preflight test (3 videos)
python scripts/run_research.py --client 3m_lighting --mode preflight
```

---

## ğŸ“ **Project Structure**

```
video-research-platform/
â”œâ”€â”€ core/                    # Client-agnostic engine
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ perception.py    # Whisper + LLaVA analysis
â”‚   â”‚   â”œâ”€â”€ extraction.py    # LLM-based JTBD extraction
â”‚   â”‚   â””â”€â”€ reporting.py     # Report generation
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ model_registry.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ video_processing.py
â”‚       â””â”€â”€ validation.py
â”‚
â”œâ”€â”€ clients/                 # Client-specific configurations
â”‚   â””â”€â”€ 3m_lighting/
â”‚       â”œâ”€â”€ config.yaml      # Search terms, JTBD framework
â”‚       â”œâ”€â”€ prompts.yaml     # LLM extraction prompts
â”‚       â”œâ”€â”€ proposals/       # Client proposals
â”‚       â””â”€â”€ reports/         # Generated reports
â”‚
â”œâ”€â”€ data/                    # Data segregated by client
â”‚   â””â”€â”€ 3m_lighting/
â”‚       â”œâ”€â”€ videos/          # Downloaded videos
â”‚       â”œâ”€â”€ analysis/        # Analysis JSON files
â”‚       â””â”€â”€ archives/        # Completed research sprints
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ models.yaml          # Model paths (Whisper, LLaVA, Llama)
â”‚
â”œâ”€â”€ scripts/                 # Runnable workflows
â”‚   â”œâ”€â”€ run_research.py      # Main entry point
â”‚   â””â”€â”€ validate_setup.py    # System validation
â”‚
â”œâ”€â”€ tests/                   # Test suite
â””â”€â”€ docs/                    # Documentation
    â”œâ”€â”€ ARCHITECTURE.md
    â”œâ”€â”€ CLIENT_SETUP.md
    â””â”€â”€ REBUILD_PLAN.md
```

---

## ğŸ”§ **Architecture**

### **Processing Pipeline**

```
1. PERCEPTION (Multimodal Analysis)
   â”œâ”€ Whisper large-v3: Audio â†’ Transcript
   â””â”€ LLaVA 7B: Frames â†’ Visual analysis (every 30s)

2. EXTRACTION (LLM-Enhanced)
   â””â”€ Llama 3.1 8B: Combined data â†’ JTBD insights
      â”œâ”€ Pain points (functional, social, emotional)
      â”œâ”€ Solutions demonstrated
      â”œâ”€ Verbatim quotes
      â”œâ”€ Golden moments
      â””â”€ Product adjacencies

3. REPORTING
   â””â”€ HTML + JSON + CSV outputs
```

### **Models Used**

| Model | Purpose | Device | Size |
|-------|---------|--------|------|
| Whisper large-v3 | Audio transcription | CPU/MPS | 1.5GB |
| LLaVA 7B | Visual analysis | Ollama | 4.5GB |
| Llama 3.1 8B | JTBD extraction | Ollama | 4.7GB |

---

## ğŸ“Š **Client Setup**

See `docs/CLIENT_SETUP.md` for detailed instructions on adding new clients.

**Quick overview:**
1. Create `clients/{client_name}/config.yaml`
2. Define JTBD framework and search terms
3. Create extraction prompts in `prompts.yaml`
4. Run: `python scripts/run_research.py --client {client_name}`

---

## ğŸ” **Current Status**

### **3M Lighting Project**

**Phase:** Preflight Complete â†’ Production Enhancement
- âœ… Preflight validated (3 videos, 11 pain points, 21 solutions)
- âœ… Client HTML report delivered
- ğŸ”„ LLM extraction layer (in progress)
- â³ Sprint 1 (50-100 videos) - pending

**Archived Results:**
- Preflight videos: `data/3m_lighting/archives/preflight_2025-10-05/`
- HTML report: `data/3m_lighting/archives/preflight_2025-10-05/analysis/PREFLIGHT_REPORT_3M_Lighting.html`

---

## ğŸ§ª **Testing**

```bash
# Run all tests
pytest tests/

# Run specific test
pytest tests/test_extraction.py

# Validate pipeline on single video
python scripts/validate_setup.py --video <video_id>
```

---

## ğŸ“ **Development**

### **Key Files**

- `core/pipeline/extraction.py` - LLM-based JTBD extraction (currently being enhanced)
- `core/pipeline/perception.py` - Whisper + LLaVA analysis
- `clients/3m_lighting/config.yaml` - 3M research configuration
- `config/models.yaml` - Model paths and settings

### **Adding Features**

1. Update relevant `core/` module
2. Add tests in `tests/`
3. Update client config if needed
4. Run validation suite

---

## ğŸ“š **Documentation**

- **Architecture:** `docs/ARCHITECTURE.md` - System design and data flow
- **Client Setup:** `docs/CLIENT_SETUP.md` - Adding new research programs
- **Rebuild Plan:** `docs/REBUILD_PLAN.md` - Latest enhancement plan

---

## ğŸ› ï¸ **Troubleshooting**

**Common Issues:**

1. **Ollama connection errors**
   ```bash
   # Verify Ollama is running
   ollama list
   # Check models are installed
   ollama pull llava:latest
   ollama pull llama3.1:8b
   ```

2. **FFmpeg not found**
   ```bash
   # macOS
   brew install ffmpeg
   # Linux
   sudo apt install ffmpeg
   ```

3. **Out of memory errors**
   - Reduce batch size in `config/models.yaml`
   - Process videos sequentially (set `num_workers: 1`)

---

## ğŸ“„ **License**

Proprietary - Offbrain Insights
